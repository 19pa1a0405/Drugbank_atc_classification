# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N6vFcQbMvnrWdaJN1ApCtxD00mWWFxz-
"""

!pip install pandas numpy scikit-learn xgboost matplotlib seaborn kagglehub shap pubchempy rdkit

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, MultiLabelBinarizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.multioutput import MultiOutputClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns
import kagglehub
import shap
import pubchempy as pcp
from rdkit import Chem
from rdkit.Chem import Descriptors
import warnings
from sklearn.metrics import multilabel_confusion_matrix
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    classification_report,
    multilabel_confusion_matrix
)

warnings.filterwarnings('ignore', category=pd.errors.DtypeWarning)

# Load the CSV
df = pd.read_csv('/content/drugbank_clean.csv')

df.head()

df.info()

df.info()

print("Missing values before cleaning:")
print(df.isnull().sum())
numerical_cols = df.select_dtypes(include=[np.number]).columns
categorical_cols = df.select_dtypes(include=['object']).columns
numerical_cols = df.select_dtypes(include=[np.number]).columns
categorical_cols = df.select_dtypes(include=['object']).columns

for col in numerical_cols:
    if df[col].isnull().sum() > 0:
        skewness = df[col].skew()

        if abs(skewness) > 1:
            # Skewed data → Median
            df[col].fillna(df[col].median(), inplace=True)
        else:
            # Normal distribution → Mean
            df[col].fillna(df[col].mean(), inplace=True)

for col in categorical_cols:
    if df[col].isnull().sum() > 0:
        df[col].fillna(df[col].mode()[0], inplace=True)

print("\nMissing values after cleaning:")
print(df.isnull().sum())

from sklearn.impute import SimpleImputer
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import hamming_loss
from collections import Counter

# helper: safe RDKit descriptor extraction
def compute_rdkit_descriptors(smiles):
    try:
        mol = Chem.MolFromSmiles(smiles)
        if mol is None:
            return {}
        descs = {
            "MolWt": Descriptors.MolWt(mol),
            "TPSA": Descriptors.TPSA(mol),
            "NumHDonors": Descriptors.NumHDonors(mol),
            "NumHAcceptors": Descriptors.NumHAcceptors(mol),
            "NumRotatableBonds": Descriptors.NumRotatableBonds(mol),
            "MolLogP": Descriptors.MolLogP(mol)
        }
        return descs
    except Exception:
        return {}

# We will keep rows where groups column contains 'approved' (case-insensitive).
df_approved = df[df['groups'].str.contains('approved', case=False, na=False)].copy()
print("Total rows:", df.shape[0])
print("Approved rows:", df_approved.shape[0])

# Keep only entries that have atc-codes
df_approved = df_approved[df_approved['atc-codes'].notna()].copy()
print("Approved rows with ATC codes:", df_approved.shape[0])

# After missing value handling
numeric_df = df.select_dtypes(include=['int64', 'float64'])
numeric_df.hist(figsize=(16, 12), bins=30, edgecolor='black')
plt.suptitle("Histogram Distribution of Numerical Features")
plt.show()

"""Correlation Heatmap"""

import seaborn as sns
plt.figure(figsize=(14, 10))
sns.heatmap(numeric_df.corr(), cmap='coolwarm', linewidths=0.5)
plt.title("Correlation Heatmap of Numerical Features")
plt.show()

#Inspect available features & simple EDA
print("Columns:", df_approved.columns.tolist())
print("\nNon-null counts (top):")
print(df_approved.count().sort_values(ascending=False).head(20))

# show distribution of number of ATC codes per drug
df_approved['atc_list'] = df_approved['atc-codes'].apply(lambda x: x.split('|') if pd.notna(x) else [])
df_approved['n_atc'] = df_approved['atc_list'].apply(len)
print("\nATC counts distribution:")
print(df_approved['n_atc'].value_counts().sort_index())
sns.histplot(df_approved['n_atc'], bins=10)
plt.title("Distribution of number of ATC codes per approved drug")
plt.xlabel("Number of ATC codes")
plt.show()

#Features selection strategy
# We'll attempt to use chemical numeric features first. If SMILES available, compute RDKit descriptors.
numerical_features = ['average-mass', 'monoisotopic-mass']  # base numeric features
if 'smiles' in df_approved.columns:
    print("SMILES column found - computing RDKit descriptors (this may take a while)...")
    # compute descriptors for rows with SMILES
    rdkit_descs = df_approved['smiles'].apply(lambda s: compute_rdkit_descriptors(s) if pd.notna(s) else {})
    # create columns
    rdkit_df = pd.DataFrame(list(rdkit_descs))
    # attach to df_approved (aligned by index)
    df_approved = pd.concat([df_approved.reset_index(drop=True), rdkit_df.reset_index(drop=True)], axis=1)
    # add descriptor names to numerical_features
    numerical_features += ['MolWt', 'TPSA', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds', 'MolLogP']

print("Numerical features candidate:", numerical_features)

#Build X (features) and y (multi-label targets)
# Use the chosen numerical features, plus create derived feature mass_difference
X = df_approved[numerical_features].copy()

# Derived features
if 'average-mass' in X.columns and 'monoisotopic-mass' in X.columns:
    X['mass_difference'] = X['average-mass'] - X['monoisotopic-mass']

# Impute numeric missing values
imputer = SimpleImputer(strategy='median')
X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns, index=X.index)

# Target: multi-label binarization of ATC codes
mlb = MultiLabelBinarizer()
y = pd.DataFrame(mlb.fit_transform(df_approved['atc_list']), columns=mlb.classes_, index=df_approved.index)

print("X shape:", X_imputed.shape)
print("y shape (labels):", y.shape)
print("Number of ATC unique labels:", len(mlb.classes_))

#Reduce extremely rare labels for stability
min_support = 10   # keep labels with at least this many samples (adjustable)
label_counts = y.sum(axis=0)
good_labels = label_counts[label_counts >= min_support].index.tolist()
print(f"Labels before filtering: {y.shape[1]}, labels with >= {min_support} samples: {len(good_labels)}")

# filter y to only good_labels
y_filtered = y[good_labels].copy()
print("y_filtered shape:", y_filtered.shape)

#Train/test split and scaling
# scale features
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X_imputed), columns=X_imputed.columns, index=X_imputed.index)

# train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_filtered, test_size=0.2, random_state=42
)
print("Train shape:", X_train.shape, y_train.shape)
print("Test shape :", X_test.shape, y_test.shape)

#Train models (One-vs-Rest). We'll train 3 models: LogisticRegression, RandomForest, XGBoost
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.multiclass import OneVsRestClassifier
import time

models = {
    "LogisticRegression": OneVsRestClassifier(LogisticRegression(max_iter=1000)),
    "RandomForest": OneVsRestClassifier(RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)),
    "XGBoost": OneVsRestClassifier(XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_jobs= -1))
}

trained = {}
for name, model in models.items():
    print(f"\nTraining {name} ...")
    t0 = time.time()
    model.fit(X_train, y_train)
    t1 = time.time()
    trained[name] = model
    print(f"Trained {name} in {t1 - t0:.1f}s")

#Evaluate models: subset accuracy, micro/macro F1, Hamming loss, sample-wise average
from sklearn.metrics import f1_score, accuracy_score

def evaluate_model(mname, model, X_test, y_test):
    y_pred = model.predict(X_test)
    # subset accuracy (exact match)
    subset_acc = accuracy_score(y_test, y_pred)
    micro_f1 = f1_score(y_test, y_pred, average='micro', zero_division=0)
    macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)
    hamming = hamming_loss(y_test, y_pred)
    samples_f1 = f1_score(y_test, y_pred, average='samples', zero_division=0)
    print(f"\n{name} Evaluation")
    print(f"Subset accuracy (exact match): {subset_acc:.4f}")
    print(f"Micro F1: {micro_f1:.4f}")
    print(f"Macro F1: {macro_f1:.4f}")
    print(f"Samples avg F1: {samples_f1:.4f}")
    print(f"Hamming loss: {hamming:.4f}")
    # Show per-label top confusion for the top-K most frequent labels
    label_support = y_test.sum(axis=0).sort_values(ascending=False)
    top_labels = label_support.head(10).index.tolist()
    print("\nTop labels by support (test):")
    print(label_support.head(10))
    # classification report for these top labels
    if len(top_labels) > 0:
        print("\nClassification report for top labels:")
        print(classification_report(y_test[top_labels], y_pred[:, [y_test.columns.get_loc(l) for l in top_labels]], zero_division=0))
    return y_pred

def plot_confusion_matrices_multilabel(
    model_name,
    y_true,
    y_pred,
    label_names,
    top_k=5
):
    label_support = y_true.sum(axis=0).sort_values(ascending=False)
    top_labels = label_support.head(top_k).index.tolist()
    label_indices = [label_names.index(lbl) for lbl in top_labels]

    mcm = multilabel_confusion_matrix(
        y_true.iloc[:, label_indices],
        y_pred[:, label_indices]
    )

    fig, axes = plt.subplots(1, top_k, figsize=(4 * top_k, 4))
    if top_k == 1:
        axes = [axes]

    for i, (label, cm) in enumerate(zip(top_labels, mcm)):
        tn, fp, fn, tp = cm.ravel()
        cm_display = np.array([[tp, fn],
                               [fp, tn]])

        sns.heatmap(
            cm_display,
            annot=True,
            fmt="d",
            cmap="Blues",
            xticklabels=["Pred 1", "Pred 0"],
            yticklabels=["True 1", "True 0"],
            ax=axes[i]
        )
        axes[i].set_title(f"{model_name}\nLabel: {label}")
        axes[i].set_xlabel("Prediction")
        axes[i].set_ylabel("Ground Truth")

    plt.tight_layout()
    plt.show()


results = {}
for name, model in trained.items():
    print("="*80)
    y_pred = evaluate_model(name, model, X_test, y_test)
    results[name] = y_pred
    plot_confusion_matrices_multilabel(
        model_name=name,
        y_true=y_test,
        y_pred=y_pred,
        label_names=list(y_test.columns),
        top_k=5
    )

#Discussion (programmatic brief summary)
# We'll print a small summary table for comparisons (subset acc, micro F1, macro F1, hamming).
summary_rows = []
for name, model in trained.items():
    y_pred = results[name]
    summary_rows.append({
        "model": name,
        "subset_acc": accuracy_score(y_test, y_pred),
        "micro_f1": f1_score(y_test, y_pred, average='micro', zero_division=0),
        "macro_f1": f1_score(y_test, y_pred, average='macro', zero_division=0),
        "samples_f1": f1_score(y_test, y_pred, average='samples', zero_division=0),
        "hamming": hamming_loss(y_test, y_pred)
    })
summary_df = pd.DataFrame(summary_rows).sort_values('micro_f1', ascending=False)
summary_df

#  SHAP interpretation for a tree model (RandomForest)
# For OneVsRestClassifier, there is an estimator per label in .estimators_
rf_model = trained.get("RandomForest")
if rf_model is None:
    print("RandomForest not trained - cannot run SHAP.")
else:
    # choose a label index with sufficient support to explain (the most frequent label)
    label_support = y_train.sum(axis=0).sort_values(ascending=False)
    top_label = label_support.index[0]
    top_label_index = list(y_train.columns).index(top_label)
    print("Explaining label:", top_label, "index:", top_label_index, "support:", label_support.iloc[0])
    estimator = rf_model.estimators_[top_label_index]  # underlying RandomForestClassifier for that label

    # use TreeExplainer (fast for tree-based models)
    explainer = shap.TreeExplainer(estimator)
    # Use a small background sample for speed
    background = X_train.sample(min(100, len(X_train)), random_state=42)
    shap_values = explainer.shap_values(X_test)
    # shap_values for binary tree estimator returns array (n_samples,) or (n_samples, n_features)? For binary it's array per class; handle accordingly.
    # For sklearn RandomForestClassifier shap returns shap_values array shaped (n_samples, n_features)
    try:
        # summary plot (global)
        shap.summary_plot(shap_values, X_test, feature_names=X_test.columns)
    except Exception as e:
        print("Plotting error:", e)
        # fallback to bar plot of mean(|shap|)
        import numpy as np
        mean_abs_shap = np.abs(shap_values).mean(axis=0)
        imp_df = pd.DataFrame({"feature": X_test.columns, "mean_abs_shap": mean_abs_shap}).sort_values('mean_abs_shap', ascending=False)
        sns.barplot(x='mean_abs_shap', y='feature', data=imp_df)
        plt.title(f"Approx SHAP importance for label {top_label}")
        plt.show()